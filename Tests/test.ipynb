{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/miniconda3/envs/esp/lib/python3.9/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkData():\n",
    "    def __init__(self, session_name, schema) -> None:\n",
    "        self.session_name = session_name\n",
    "        self.schema = schema\n",
    "        self.spark = SparkSession.builder.appName(self.session_name).getOrCreate()\n",
    "        self.dataframe = self.spark.createDataFrame(self.spark.sparkContext.emptyRDD(), schema)\n",
    "    \n",
    "\n",
    "    def add_line(self,line):\n",
    "        new_line = self.spark.createDataFrame(line, self.schema)\n",
    "        self.dataframe = self.dataframe.union(new_line)\n",
    "    \n",
    "    def save_parquet(self, path_save):\n",
    "        self.dataframe.write.mode(\"overwrite\").parquet(path_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina o esquema do DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"data\", StringType(), True),\n",
    "    StructField(\"temperatura\", FloatType(), True),\n",
    "    StructField(\"umidade\", FloatType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/03 14:48:59 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.100.4 instead (on interface wlp4s0)\n",
      "24/09/03 14:48:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/03 14:48:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/03 14:49:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "myspark = SparkData(session_name='teste',schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 40.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(0,100)):\n",
    "    temperatura = random.random()\n",
    "    umidade = random.random()\n",
    "    myspark.add_line(line=[( \"2024-09-03\", temperatura, umidade )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:================================================>    (459 + 12) / 500]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+\n",
      "|      data| temperatura|    umidade|\n",
      "+----------+------------+-----------+\n",
      "|2024-09-03|  0.30986512|   0.780457|\n",
      "|2024-09-03|   0.7466063| 0.86541337|\n",
      "|2024-09-03|  0.58183557| 0.04761888|\n",
      "|2024-09-03|  0.29605022|  0.4119609|\n",
      "|2024-09-03| 0.029822059|   0.602383|\n",
      "|2024-09-03|  0.36185965| 0.17977515|\n",
      "|2024-09-03|  0.02846358| 0.48384297|\n",
      "|2024-09-03|  0.02631612|  0.9758619|\n",
      "|2024-09-03|  0.37667465| 0.65551656|\n",
      "|2024-09-03|  0.55032027|  0.5316353|\n",
      "|2024-09-03|   0.1181135|0.078024514|\n",
      "|2024-09-03|  0.15258212| 0.12880369|\n",
      "|2024-09-03|  0.19670658| 0.95416725|\n",
      "|2024-09-03|   0.8332387|  0.8670788|\n",
      "|2024-09-03|  0.96362346| 0.80392283|\n",
      "|2024-09-03|   0.6376718| 0.77230304|\n",
      "|2024-09-03|   0.5932955| 0.52701753|\n",
      "|2024-09-03|  0.67832094| 0.47431764|\n",
      "|2024-09-03|   0.8677575| 0.52518415|\n",
      "|2024-09-03|  0.74355125| 0.80102795|\n",
      "|2024-09-03|    0.821113|  0.8384065|\n",
      "|2024-09-03|  0.57701856| 0.20602085|\n",
      "|2024-09-03|   0.6559576|  0.8865137|\n",
      "|2024-09-03|  0.35648608| 0.24362278|\n",
      "|2024-09-03|  0.46175328| 0.66146284|\n",
      "|2024-09-03|   0.2682483|  0.5598245|\n",
      "|2024-09-03|  0.97937804| 0.74498576|\n",
      "|2024-09-03|  0.16204496| 0.69201595|\n",
      "|2024-09-03|  0.84467715|0.089298576|\n",
      "|2024-09-03| 3.479883E-4| 0.82911414|\n",
      "|2024-09-03|   0.8887048|  0.4261828|\n",
      "|2024-09-03|   0.5896159| 0.23379947|\n",
      "|2024-09-03|  0.07111581|  0.6720314|\n",
      "|2024-09-03|   0.5337465|  0.7521609|\n",
      "|2024-09-03|   0.7437366| 0.07845341|\n",
      "|2024-09-03|  0.33704466|  0.0730747|\n",
      "|2024-09-03|  0.13332872| 0.13581228|\n",
      "|2024-09-03| 0.110407606|  0.4708319|\n",
      "|2024-09-03|  0.54425734| 0.14867981|\n",
      "|2024-09-03|   0.4568064|  0.1486591|\n",
      "|2024-09-03|  0.48122397| 0.46937612|\n",
      "|2024-09-03|  0.42986804| 0.19858128|\n",
      "|2024-09-03|  0.04771786|0.022969402|\n",
      "|2024-09-03|3.8622853E-5|  0.6825218|\n",
      "|2024-09-03|  0.52723014|  0.8234881|\n",
      "|2024-09-03| 0.011658673|  0.9679298|\n",
      "|2024-09-03|  0.26575178|   0.548076|\n",
      "|2024-09-03|   0.8032789|  0.7731184|\n",
      "|2024-09-03|  0.07333255|0.044916138|\n",
      "|2024-09-03|  0.66402495| 0.65706515|\n",
      "+----------+------------+-----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "myspark.dataframe.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:============================================>        (420 + 12) / 500]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+\n",
      "|      data| temperatura|    umidade|\n",
      "+----------+------------+-----------+\n",
      "|2024-09-03|  0.30986512|   0.780457|\n",
      "|2024-09-03|   0.7466063| 0.86541337|\n",
      "|2024-09-03|  0.58183557| 0.04761888|\n",
      "|2024-09-03|  0.29605022|  0.4119609|\n",
      "|2024-09-03| 0.029822059|   0.602383|\n",
      "|2024-09-03|  0.36185965| 0.17977515|\n",
      "|2024-09-03|  0.02846358| 0.48384297|\n",
      "|2024-09-03|  0.02631612|  0.9758619|\n",
      "|2024-09-03|  0.37667465| 0.65551656|\n",
      "|2024-09-03|  0.55032027|  0.5316353|\n",
      "|2024-09-03|   0.1181135|0.078024514|\n",
      "|2024-09-03|  0.15258212| 0.12880369|\n",
      "|2024-09-03|  0.19670658| 0.95416725|\n",
      "|2024-09-03|   0.8332387|  0.8670788|\n",
      "|2024-09-03|  0.96362346| 0.80392283|\n",
      "|2024-09-03|   0.6376718| 0.77230304|\n",
      "|2024-09-03|   0.5932955| 0.52701753|\n",
      "|2024-09-03|  0.67832094| 0.47431764|\n",
      "|2024-09-03|   0.8677575| 0.52518415|\n",
      "|2024-09-03|  0.74355125| 0.80102795|\n",
      "|2024-09-03|    0.821113|  0.8384065|\n",
      "|2024-09-03|  0.57701856| 0.20602085|\n",
      "|2024-09-03|   0.6559576|  0.8865137|\n",
      "|2024-09-03|  0.35648608| 0.24362278|\n",
      "|2024-09-03|  0.46175328| 0.66146284|\n",
      "|2024-09-03|   0.2682483|  0.5598245|\n",
      "|2024-09-03|  0.97937804| 0.74498576|\n",
      "|2024-09-03|  0.16204496| 0.69201595|\n",
      "|2024-09-03|  0.84467715|0.089298576|\n",
      "|2024-09-03| 3.479883E-4| 0.82911414|\n",
      "|2024-09-03|   0.8887048|  0.4261828|\n",
      "|2024-09-03|   0.5896159| 0.23379947|\n",
      "|2024-09-03|  0.07111581|  0.6720314|\n",
      "|2024-09-03|   0.5337465|  0.7521609|\n",
      "|2024-09-03|   0.7437366| 0.07845341|\n",
      "|2024-09-03|  0.33704466|  0.0730747|\n",
      "|2024-09-03|  0.13332872| 0.13581228|\n",
      "|2024-09-03| 0.110407606|  0.4708319|\n",
      "|2024-09-03|  0.54425734| 0.14867981|\n",
      "|2024-09-03|   0.4568064|  0.1486591|\n",
      "|2024-09-03|  0.48122397| 0.46937612|\n",
      "|2024-09-03|  0.42986804| 0.19858128|\n",
      "|2024-09-03|  0.04771786|0.022969402|\n",
      "|2024-09-03|3.8622853E-5|  0.6825218|\n",
      "|2024-09-03|  0.52723014|  0.8234881|\n",
      "|2024-09-03| 0.011658673|  0.9679298|\n",
      "|2024-09-03|  0.26575178|   0.548076|\n",
      "|2024-09-03|   0.8032789|  0.7731184|\n",
      "|2024-09-03|  0.07333255|0.044916138|\n",
      "|2024-09-03|  0.66402495| 0.65706515|\n",
      "+----------+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "myspark.dataframe.limit(50).show(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
